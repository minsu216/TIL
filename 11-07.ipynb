{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"j9ZCZGBzQf6z"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpM4rk3CQqHg"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MhZ_xnYQtxU"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6EAirfHh6O5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yg2aHRavh6RA"},"outputs":[],"source":["text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n","그의 말이 법이다\\n\n","가는 말이 고와야 오는 말이 곱다\\n\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQ2pXmaGf0UR"},"outputs":[],"source":["tokenizer=Tokenizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Id9zI0dYhrD5"},"outputs":[],"source":["tokenizer.fit_on_texts([text])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5c_Ve_GphtzI"},"outputs":[],"source":["vocab_size=len(tokenizer.word_index)+1\n","print('단어 집합의 크기 : %d' % vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkQpNODah6s6"},"outputs":[],"source":["tokenizer.index_word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwwlGO1GiCVH"},"outputs":[],"source":["text.split('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEbXX_qoj1Uh"},"outputs":[],"source":["tokenizer.texts_to_sequences(['경마장에 있는 말이 뛰고 있다'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKsyqfqmjbJg"},"outputs":[],"source":["sequences = list()\n","for line in text.split('\\n'): # 줄바꿈 문자를 기준으로 문장 토큰화\n","    encoded = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(encoded)): # [2,3,1,4,5]\n","        sequence = encoded[:i+1] # [2,3] , [2,3,1] ..\n","        sequences.append(sequence) # [[2,3],[2,3,1]....]\n","\n","print('학습에 사용할 샘플의 개수: %d' % len(sequences))"]},{"cell_type":"markdown","metadata":{"id":"FFh-8P6Vll5J"},"source":["2 -> model -> 3\n","2,3 -> model -> 1  가장 마지막에 들어간 것이 타겟이라고 생각 ( y : target data)\n","입력길이를 얼마나 할지 고민해야된다. (1 ~ 5까지)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQa5_MjQjwMw"},"outputs":[],"source":["sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLPanr0tlODf"},"outputs":[],"source":["max_len = max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n","print('샘플의 최대 길이 : {}'.format(max_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EfGX9UN9cyv"},"outputs":[],"source":["sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYzxkAiO9hOW"},"outputs":[],"source":["sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPpP057d9hvO"},"outputs":[],"source":["sequences = np.array(sequences)\n","X = sequences[:,:-1]\n","y = sequences[:,-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4uG4I_jATIV"},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDuRWy3pAVI8"},"outputs":[],"source":["y = to_categorical(y, num_classes=vocab_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqCg0iKuAwvt"},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4x340WtiAxNF"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6zcpbu4DnnE"},"outputs":[],"source":["embedding_dim = 10\n","hidden_units = 32\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","model.add(SimpleRNN(hidden_units))\n","model.add(Dense(vocab_size, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, y, epochs=200, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcvhsxzgDoa0"},"outputs":[],"source":["embedding_dim = 10 # 임베딩 차원 : 10차원\n","hidden_units = 32\n","# 임베딩? 단엉를 벡터공간 표현 -> 임베딩 벡터 공간 : 11차원(단어 의 종류 개수)+1 = 12차원\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim)) # 12차원의 데이터를 10차원 공간 데이터로 표혀해라!\n","\n","model.add(SimpleRNN(hidden_units))\n","model.add(Dense(vocab_size, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, y, epochs=200, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m40xMh4aDrCM"},"outputs":[],"source":["def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n","    init_word = current_word\n","    sentence = ''\n","\n","    # n번 반복\n","    for _ in range(n):\n","        # 현재 단어에 대한 정수 인코딩과 패딩\n","        encoded = tokenizer.texts_to_sequences([current_word])[0]\n","        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n","        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n","        result = model.predict(encoded, verbose=0)\n","        result = np.argmax(result, axis=1)\n","\n","        for word, index in tokenizer.word_index.items():\n","            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n","            if index == result:\n","                break\n","\n","        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n","        current_word = current_word + ' '  + word\n","\n","        # 예측 단어를 문장에 저장\n","        sentence = sentence + ' ' + word\n","\n","    sentence = init_word + sentence\n","    return sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zr3-T9y4DtF7"},"outputs":[],"source":["print(sentence_generation(model, tokenizer, '경마장에', 4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSQvcAY0DuNz"},"outputs":[],"source":["def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n","    current_word = '경마장에 있는'\n","    init_word = current_word\n","    sentence = ''\n","    encoded = tokenizer.text_to_sequences([current_word])[0]\n","    print(encoded)# [2,3]\n","    encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n","    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n","    result = model.predict(encoded, verbose=0)\n","    result = np.argmax(result, axis=1)\n","\n","    # n번 반복\n","    for _ in range(n):\n","        # 현재 단어에 대한 정수 인코딩과 패딩\n","\n","\n","        for word, index in tokenizer.word_index.items():\n","            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n","            if index == result:\n","                break\n","\n","        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n","        current_word = current_word + ' '  + word\n","\n","        # 예측 단어를 문장에 저장\n","        sentence = sentence + ' ' + word\n","\n","    sentence = init_word + sentence\n","    return sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbDCKfTDUcYQ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from string import punctuation\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyAkcwpNPwBZ"},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ArticlesApril2018.csv')\n","df.head\n","#head 라인 열에있는 자료들을 추출한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6nUjkmjUhRO"},"outputs":[],"source":["print('열의 개수: ',len(df.columns))\n","print(df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xc_MWAFUiuJ"},"outputs":[],"source":["print(df['headline'].isnull().values.any())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j44rQtqpUnHH"},"outputs":[],"source":["headline=[]\n","headline.extend(list(df.headline.values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYMDtqF2VACW"},"outputs":[],"source":["headline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6KKMVEbVPO-"},"outputs":[],"source":["def repreprocessing(raw_sentence): #\n","    preproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",'ignore')\n","    # 구두점 제거와 동시에 소문자화\n","    return ''.join(word for word in preproceseed_sentence if word not in punctuation).lower()\n","\n","preprocessed_headline = [repreprocessing(x) for x in headline]\n","preprocessed_headline[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWvf__ZSVp-u"},"outputs":[],"source":["punctuation #"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFY2eJqlVvUu"},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(preprocessed_headline)\n","vocab_size = len(tokenizer.word_index) + 1 # 3495개\n","print('단어 집합의 크기 : %d' % vocab_size)# 단어.. 집합의 전체 크기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-5jfigDWHKW"},"outputs":[],"source":["sequences = list()\n","\n","for sentence in preprocessed_headline:\n","\n","    # 각 샘플에 대한 정수 인코딩\n","    encoded = tokenizer.texts_to_sequences([sentence])[0]\n","    for i in range(1, len(encoded)):\n","        sequence = encoded[:i+1]\n","        sequences.append(sequence)\n","\n","sequences[:11]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yo-f8bONWpYV"},"outputs":[],"source":["index_to_word = {}\n","for key, value in tokenizer.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n","    index_to_word[value] = key"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SUqZpksW5YT"},"outputs":[],"source":["index_to_word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7eHsg76W6bB"},"outputs":[],"source":["max_len = max(len(l) for l in sequences)\n","print('샘플의 최대 길이 : {}'.format(max_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AF_hBqzcX57d"},"outputs":[],"source":["sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkVbt_7PYUm1"},"outputs":[],"source":["sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRzfg696YWpx"},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDl42jJxYKK1"},"outputs":[],"source":["sequences= np.array(sequences)\n","X= sequences[:,:-1] #\n","y= sequences[:,-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"960iKwHEYNxd"},"outputs":[],"source":["vocab_size # 차원이 3495 차원이다!\n","\n","# ex ) 1025 -> 0000......0010000000000.... 00"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaKGQ4HpYvKH"},"outputs":[],"source":["y = to_categorical(y, num_classes=vocab_size)\n","y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMmaOQaKY72_"},"outputs":[],"source":["y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GF68ANEWY8-0"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Dense, LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfjOT9zUeKET"},"outputs":[],"source":["embedding_dim = 10\n","hidden_units = 128\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim)) # 3494 -> 10차원으로\n","model.add(LSTM(hidden_units))# lstm 셀 출력 : 128차원\n","model.add(Dense(vocab_size, activation='softmax')) # Dense Fully connected 레이어! ,\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, y, epochs=200, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57N6KFnleLDc"},"outputs":[],"source":["print(sentence_generation(model, tokenizer, 'i', 10))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNoIOudj36Q+SZw63ptSkCM","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
